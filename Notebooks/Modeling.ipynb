{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import spacy\n",
    "import pickle\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_reviews_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>0</td>\n",
       "      <td>work with one of the good shakespeare source t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>well tremor i the original start off in and i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>0</td>\n",
       "      <td>ouch this one be a bit painful to sit through ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>0</td>\n",
       "      <td>-PRON- have see some crappy movie in -PRON- li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>0</td>\n",
       "      <td>carrier follow the exploit of two guy and tw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment  \\\n",
       "0  Working with one of the best Shakespeare sourc...          0   \n",
       "1  Well...tremors I, the original started off in ...          0   \n",
       "2  Ouch! This one was a bit painful to sit throug...          0   \n",
       "3  I've seen some crappy movies in my life, but t...          0   \n",
       "4  \"Carriers\" follows the exploits of two guys an...          0   \n",
       "\n",
       "                                     cleaned_reviews  \n",
       "0  work with one of the good shakespeare source t...  \n",
       "1  well tremor i the original start off in and i ...  \n",
       "2  ouch this one be a bit painful to sit through ...  \n",
       "3  -PRON- have see some crappy movie in -PRON- li...  \n",
       "4    carrier follow the exploit of two guy and tw...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'pron', '-PRON-']\n"
     ]
    }
   ],
   "source": [
    "# create stopwords and update as spacy lemmatizer converts all pronouns (i, we, us, etc..) to -PRON-\n",
    "stopwords_list = stopwords.words('english')+['pron', '-PRON-']\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train test split our cleaned reviews and vectorize using specific stopwords\n",
    "def split_vectorize(predictor, target, vectorizer, stopwords):\n",
    "    #TRAIN/TEST SPLIT\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictor, target, random_state=333)\n",
    "    \n",
    "    # vectorize our predictor column, n_gram range was chosen as we found having groups of words\n",
    "    # increased our score, min_df of 7 got rid of many nonsensical tokens such as 'zzzhf' and kept\n",
    "    # dataframe at a manageable size (around 50,000 columns)\n",
    "    vect = vectorizer(ngram_range=(1, 3), min_df = 7, stop_words=stopwords)\n",
    "    \n",
    "    #fit/transform train set and transform tet set\n",
    "    X_train=vect.fit_transform(X_train)\n",
    "    X_test=vect.transform(X_test)\n",
    "    \n",
    "    # make into a dataframe with the ngrams as column names\n",
    "    X_train = pd.DataFrame(X_train.toarray(), columns=vect.get_feature_names())\n",
    "    X_test = pd.DataFrame(X_test.toarray(), columns=vect.get_feature_names())\n",
    "    \n",
    "    #return the \n",
    "    return X_train, X_test, y_train, y_test, vectorizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vectorize our cleaned reviews using CountVectorizer\n",
    "X_train, X_test, y_train, y_test, BOW_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], CountVectorizer, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle our BOW vectorizer for use with holdout set\n",
    "pickle_out = open(\"BOW_vect.pickle\",\"wb\")\n",
    "pickle.dump(BOW_vect, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy score:  0.9285866666666667\n",
      "Test Accuracy score:  0.8824\n",
      "\n",
      "Train F1 score:  0.9293366404559608\n",
      "Test F1 score:  0.884124231436229\n",
      "\n",
      "Confusion matrix test set: \n",
      " [[0.43376 0.06496]\n",
      " [0.05264 0.44864]]\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATE LOGISTIC REGRESSION\n",
    "logreg = LogisticRegression(C = .01, random_state = 333)\n",
    "\n",
    "\n",
    "#fit to training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_log_train = logreg.predict(X_train)\n",
    "y_pred_log_test = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_log_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_log_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_log_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_log_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_log_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix, without normalization\n",
      "[[2711  406]\n",
      " [ 329 2804]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVsAAAEmCAYAAADMczPyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5yU1dnG8d/FIogixYIKFjQixkZT7IqxYUnQRI1K7BFje2M0JsQYsQRjEkuiMRpURKOixkqUCKiJJRFBEAuxAJaIohQBGyDg/f7xnMVh3TLA7pTd6+vn+czMeco5syv3nrnnPOcoIjAzs4bVrNgNMDNrChxszcwKwMHWzKwAHGzNzArAwdbMrAAcbM3MCsDB1uqVpFaS/i5pvqS/rcJ1+ksaXZ9tKxZJe0h6vdjtsOKSx9k2TZKOAc4BtgI+ASYBgyPimVW87rHAWcCuEbFklRta4iQF0CUipha7LVba3LNtgiSdA/wBuAxYH9gE+DPQrx4uvynwRlMItPmQ1LzYbbASERHemtAGtAU+BY6o5ZiWZMH4/bT9AWiZ9vUBpgPnAjOBGcCJad/FwBfA4lTHycBFwO051+4MBNA8vT4BeJOsd/0W0D+n/Jmc83YFxgPz0+OuOfv+BVwK/DtdZzSwbg3vrbL9P8tp/6HAQcAbwEfA+TnH9waeBealY/8EtEj7nkrv5bP0fr+fc/2fAx8Af60sS+d8I9XRM73uCMwG+hT7/w1vDbu5Z9v07AKsDjxQyzG/BHYGugPdyALOBTn7NyAL2p3IAup1ktpHxCCy3vLdEdE6Im6urSGS1gSuAQ6MiLXIAuqkao5bG3gkHbsOcBXwiKR1cg47BjgR6AC0AH5aS9UbkP0MOgEXAjcCPwB6AXsAF0raPB27FPgJsC7Zz24f4HSAiNgzHdMtvd+7c66/Nlkvf0BuxRExjSwQ3yFpDeAWYFhE/KuW9loj4GDb9KwDzI7aP+b3By6JiJkRMYusx3pszv7Faf/iiBhJ1qvrupLt+RLYVlKriJgREZOrOeZgYEpE/DUilkTEcOA14Ns5x9wSEW9ExALgHrI/FDVZTJafXgzcRRZI/xgRn6T6JwPbA0TEhIgYm+p9G/gLsFce72lQRCxK7VlORNwITAGeAzYk++NmjZyDbdMzB1i3jlxiR+CdnNfvpLJl16gSrD8HWq9oQyLiM7KP3j8CZkh6RNJWebSnsk2dcl5/sALtmRMRS9PzymD4Yc7+BZXnS9pS0sOSPpD0MVnPfd1arg0wKyIW1nHMjcC2wLURsaiOY60RcLBtep4FFpLlKWvyPtlH4EqbpLKV8RmwRs7rDXJ3RsSoiNiPrIf3GlkQqqs9lW16byXbtCKuJ2tXl4hoA5wPqI5zah3iI6k1WR78ZuCilCaxRs7BtomJiPlkecrrJB0qaQ1Jq0k6UNLv0mHDgQskrSdp3XT87StZ5SRgT0mbSGoL/KJyh6T1JX0n5W4XkaUjllZzjZHAlpKOkdRc0veBrYGHV7JNK2It4GPg09TrPq3K/g+Bzb92Vu3+CEyIiB+S5aJvWOVWWslzsG2CIuIqsjG2FwCzgHeBM4EH0yG/Bp4HXgJeBiamspWpawxwd7rWBJYPkM3IRjW8T/YN/V6kL5+qXGMOcEg6dg7ZSIJDImL2yrRpBf2U7Mu3T8h63XdX2X8RcKukeZKOrOtikvoBfclSJ5D9HnpK6l9vLbaS5JsazMwKwD1bM7MCcLA1MysAB1szswJwsDUzKwBPkpEHrbZGqGXbYjfDqujWtVPdB1lRTJo4YXZErFdf16tos2nEkq/djPc1sWDWqIjoW1/11icH2zyoZVtabn9CsZthVTz5xOBiN8Fq0LZVRdU7/lZJLFlAy651jqxj4aTr6rq7r2gcbM2s9EnQrKLYrVglDrZmVh5U3l8xOdiaWXlQXVNSlDYHWzMrA3LP1syswQnnbM3MGp6cRjAzKwinEczMCsA9WzOzBuZxtmZmBeI0gplZQ/PQLzOzwmjmnK2ZWcPyOFszs0JwGsHMrDA89MvMrADcszUza2AeZ2tmViBOI5iZNTR/QWZmVhhl3rMt7z8VZtY0SNCsed1bnZfRxpL+KelVSZMl/TiVXyTpPUmT0nZQzjm/kDRV0uuSDsgp75vKpkoaWFfd7tmaWXmon57tEuDciJgoaS1ggqQxad/VEXHF8lVqa+AoYBugI/CYpC3T7uuA/YDpwHhJIyLivzVV7GBrZuWhHnK2ETEDmJGefyLpVaBTLaf0A+6KiEXAW5KmAr3TvqkR8SaApLvSsTUGW6cRzKw8SHVvK3Q5dQZ6AM+lojMlvSRpqKT2qawT8G7OadNTWU3lNXKwNbPSVznOtq4N1pX0fM42oPrLqTVwH3B2RHwMXA98A+hO1vO9svLQak6PWspr5DSCmZUF5ddznR0RO9RxndXIAu0dEXE/QER8mLP/RuDh9HI6sHHO6RsB76fnNZVXyz1bMyt5Igu2dW11Xic76Gbg1Yi4Kqd8w5zDDgNeSc9HAEdJailpM6ALMA4YD3SRtJmkFmRfoo2orW73bM2s9InqP7ivuN2AY4GXJU1KZecDR0vqTpYKeBs4FSAiJku6h+yLryXAGRGxFEDSmcAooAIYGhGTa6vYwdbMyoBo1qxeRiM8Q/Vhe2Qt5wwGBldTPrK286pysDWzspBnzrZkOdiaWVlwsDUza2j1l7MtGgdbMyt5qqecbTE52JpZWXAawcysABxszcwamnO2ZmYNzzlbM7MCcRrBzKwQyjvWOtiaWRmQe7ZmZgXhnK2ZWQMT+U2hWMocbMvcRh3actOvjmD9ddbiyy+DoSPGcd09/+GvlxxNl03WBaDdWq2Y98kCdj7hWtZuswZ3Dj6GXt/ciNtHTuQnV301BedFp+5P/749aLdWK9bb96LivKFGbunSpey1W286duzIPff/nbfffouTjj2GuXM/olv3HgwZehstWrQA4P577+HywZcgiW23256bb72jyK0vsvKOtQ625W7J0i8ZeO1IJr3xPq3XaMF/hp7F4+OmcuyFw5cdc/lZBzH/04UALPxiMZfcOIatN1+fbTbfYLlrjXzmVW6491levvvcgr6HpuT6P11D165b8cknHwMw6JcDOf2sH3P4kUdx9lmncduwm/nhgNOYNnUKV13xW0Y98TTt27dn1syZRW55kTWCnG15J0GMD+Z8wqQ3stU4Pv38C157ZyYd12uz3DHf+9Z23DPmRQA+X7iY/7z0Dgu/WPK1a42b/C4fzPmk4RvdRL03fTqjHh3JcSeeDEBE8NST/+TQ7x4OwDH9j+ORvz8EwLChN3HKqafRvn227uB6HToUp9ElpFmzZnVupay0W2crZJMN2tG9S0fGT/5q0c/dunfmw48+Zdr0OUVsmQEMPO8nXDL48mVB4aM5c2jbth3Nm2cfMDt22ogZ72d/OKdNeYOpU6aw/957sM+eu/LY6EeL1u6SoTy2ElbwYCtpqaRJkl6R9DdJa6zENW6StHV6fn6Vff+pr7aWkzVbtWD4ZT/gvD8+zCefL1pWfuS+3fjbYy8WsWUG8OjIh1mvQwd69Oy1rCzi64uxVn5UXrJ0CW9OncIjo5/g5tvu4KzTBjBv3ryCtbcU1ccaZMVUjJ7tgojoHhHbAl8AP1rRC0TEDyPiv+nl+VX27VoPbSwrzSuaMfyy/tw9ehIPPfnVMkgVFc3o12cb7n3spSK2zgDGPvsf/vHw39mu6+acdNwxPPWvfzLwvJ8wf/48lizJUjrvvzedDTbM1h3s2GkjDvr2d1httdXo3HkzttiyK9OmTinmWyiqfAKtg23tnga2AJB0TurtviLp7FS2pqRHJL2Yyr+fyv8laQdJlwOtUk/5jrTv0/R4t6SDKiuSNEzS9yRVSPq9pPGSXpJ0aqHfdH274fzv8frbs7jmrmeWK//WDlvwxjuzeG/Wx0VqmVW66NLLeHXa/3j59TcZetud7Nlnb24adjt77NmHB++/F4A777iNgw7pB8Ah3+7H00/+C4A5s2czbcobbLbZ5sVqfklwznYlSWoOHEi2ymUv4ERgJ2Bn4BRJPYC+wPsR0S31hJdLXEXEQL7qKfevUsVdQGVwbgHsQ7Y428nA/IjYEdgx1bVZQ73Phrbr9pvS/8Ce7NVrc8YOO4uxw87igF26AnDEvtsv+2Is12v3/Yzf/t/B/OCgnkx9cCBbdc6+fBl8el+mPjiQNVZfjakPDuSXJ+9T0PfSFF08+HKuu+YPdN9mSz6aM4fjTjgJgH32O4C1116H3j225ZC++3DJZb9l7XXWKXJri6zMc7aqLm/UoBVKS4GX08ungXOB04B1IuLCdMylwCyy4DoKuAd4OCKeTvv/Bfw0Ip6X9GlEtM65/qcR0VrS6sAUsp5zX+DIiOgv6V5ge+DzdEpb4NSIGF2lnQOAAQC0aNNr9V6n1+8PwlbZh098bcFTKxFtW1VMiIgd6ut6LdfvEp36/7HO4966+uB6rbc+FWOc7YKI6J5boBqSLRHxRur1HgT8RtLoiLgkn0oiYmEKygeQ9XArB54KOCsiRtVx/hBgCECz1hsW9i+SmS3P42zrzVPAoZLWkLQmcBjwtKSOwOcRcTtwBdCzmnMXS1qthuveRZae2IOsh0x6PK3yHElbpjrNrERl89nWvZWykriDLCImShoGjEtFN0XEC5IOAH4v6UtgMVm6oaohwEuSJlaTtx0N3AaMiIgvKq8NdAYmph71LODQen1DZlbvyrxjW/hgm5tfrVJ+FXBVlbJRfNUjzS3vk/P858DPq7t+RCwG1qly7pdkw8WWGzJmZqWt3NMIJdGzNTOrldyzNTNrcAIqKso72jrYmllZcBrBzKyhOY1gZtbwhHu2ZmYFUPrjaOviYGtmZaHce7alcgeZmVnNUs62rq3Oy0gbS/qnpFclTZb041S+tqQxkqakx/apXJKukTQ1zRLYM+dax6fjp0g6vq66HWzNrORV5mzrYT7bJcC5EfFNshkGz0gLEQwEHo+ILsDj6TVkMxN2SdsA4HqytqwNDCKbqbA3MKgyQNfEwdbMykJ9zI0QETMiYmJ6/gnwKtAJ6Afcmg67la9u4e8H3BaZsUA7SRuSTXA1JiI+ioi5wBiy2QVr5JytmZWFPFO260p6Puf1kDSDXzXXU2egB/AcsH5EzIAsIEuqXGGzE/BuzmnTU1lN5TVysDWz0pf/FIuz85nPVlJr4D7g7Ij4uJZrV7cjaimvkdMIZlbyspztqn9BBpCmV70PuCMi7k/FH6b0AOlxZiqfDmycc/pGwPu1lNfIwdbMykD9zGebplW9GXg1zTRYaQRQOaLgeOChnPLj0qiEncmW1JpBNhvh/pLapy/G9qeaGQpzOY1gZmWhnsbZ7gYcS7b24aRUdj5wOXCPpJOB/wFHpH0jyVaKmUq2lNaJABHxUVq+a3w67pKI+Ki2ih1szaz01dPcCBHxDDUvDfm1FU4jW6TxjBquNRQYmm/dDrZmVvI8N4KZWYF4bgQzswJwz9bMrKF5Plszs4Yn8p77oGQ52JpZWahorDlbSW1qOzEiPq7/5piZVa/MO7a19mwn8/V7gCtfB7BJA7bLzGwZ5T83QsmqMdhGxMY17TMzK7QyzyLkNzeCpKMknZ+ebySpV8M2y8xsefUxN0Ix1RlsJf0J2JvsfmLI7g++oSEbZWaWS6QRCXX8V8ryGY2wa0T0lPQCLJuAoUUDt8vMbDkl3nGtUz7BdrGkZqSJcSWtA3zZoK0yM8uV/xpjJSufYHsd2US760m6GDgSuLhBW2VmlkM04nG2lSLiNkkTgH1T0RER8UrDNsvMbHll3rHN+w6yCmAxWSrBqzuYWcGVexohn9EIvwSGAx3J1tm5U9IvGrphZmaV8ll/rNRjcT492x8AvSLicwBJg4EJwG8asmFmZrkqSj2a1iGfYPtOleOaA282THPMzKpX7mmE2iaiuZosR/s5MFnSqPR6f+CZwjTPzCwbjVDmgxFq7dlWjjiYDDySUz624ZpjZlaNxjzONiJuLmRDzMxqU+pzH9SlzpytpG8Ag4GtgdUryyNiywZsl5nZMo0hjZDPmNlhwC1k7/dA4B7grgZsk5nZ1yilEmrbSlk+wXaNiBgFEBHTIuICslnAzMwKRnlspSyfoV+LlP3JmCbpR8B7QIeGbZaZ2VekJjA3AvAToDXwf2S527bASQ3ZKDOzqko9TVCXfCaieS49/YSvJhA3MyuoMo+1td7U8ABpDtvqRMR3G6RFZmZVCNGszKNtbT3bPxWsFSWuR9dO/PspTwVRatrveGaxm2CFokY8zjYiHi9kQ8zMalPuc7vmO5+tmVnRiPL/gqzc/1iYWRPRTHVvdZE0VNJMSa/klF0k6T1Jk9J2UM6+X0iaKul1SQfklPdNZVMlDcyr/fm+UUkt8z3WzKw+VY6zrWvLwzCgbzXlV0dE97SNzOrU1sBRwDbpnD9LqpBUQbY244Fk0xgcnY6tVT4rNfSW9DIwJb3uJunafN6VmVl9qY+ebUQ8BXyUZ5X9gLsiYlFEvAVMBXqnbWpEvBkRX5BNX9CvzvbnUeE1wCHAnNTYF/HtumZWYHkui7OupOdztgF5Xv5MSS+lNEP7VNYJeDfnmOmprKbyWuXzBVmziHinSnJ6aR7nmZnVi2zWr7zSBLMjYocVvPz1wKVk9xVcClxJdpdsdRXWtOhtjfckVMon2L4rqTcQKVdxFvBGHueZmdWbigYajBARH1Y+l3Qj8HB6OR3YOOfQjYD30/OaymuUTxrhNOAcYBPgQ2DnVGZmVhBSdgdZXdtKXnvDnJeH8dUqNSOAoyS1lLQZ0AUYB4wHukjaTFILsi/RRtRVTz5zI8xMFzMzK5r6GGYraTjQhyy3Ox0YBPSR1J0sFfA2cCpAREyWdA/wX2AJcEZELE3XORMYBVQAQyNicl1157NSw41Uk4+IiHwTz2Zmq6w+7taNiKOrKa5xCbCIGEw222HV8pHAyBWpO5+c7WM5z1cn62a/W8OxZmb1TjSB+Wwj4u7c15L+CoxpsBaZmVWV5zjaUrYycyNsBmxa3w0xM6uNSn7hm9rlk7Ody1c522Zkd1/kdS+wmVl9aAyr69YabNPaY93I1h0D+DIi6hy8a2ZW38o9Z1vrONsUWB+IiKVpc6A1s4Kr7Nmu6twIxZTPTQ3jJPVs8JaYmdUkj3kRSn2629rWIGseEUuA3YFTJE0DPiP7IxMR4QBsZgXTmNcgGwf0BA4tUFvMzKqVjbMtditWTW3BVgARMa1AbTEzq4Fo1oiHfq0n6ZyadkbEVQ3QHjOzr8nWICt2K1ZNbcG2AmhN9XM6mpkVThmMNqhLbcF2RkRcUrCWmJnVoLHPjVDe78zMGpXGPBphn4K1wsysDmUea2sOthGR7wqUZmYNSuR3B1YpW5lZv8zMCkuNO41gZlYSVmB13ZLlYGtmZaG8Q62DrZmViTLv2DrYmlnpE6KizKOtg62ZlQU52JqZNbzyDrUOtmZWDuSerZlZgxM4Z2tmVgjlHWodbM2sTJR5x9bB1sxKXzY3QnlHWwdbMysD8u26ZmaFUOax1sHWzEqf0whmZoWg8u/Zlvt8vGbWRDST6tzqImmopJmSXskpW1vSGElT0mP7VC5J10iaKuklST1zzjk+HT9F0vF5tX8l3rOVqIULF7L7Lr3p3bMbPbttw6UXDwLghGP7s/02XenVfVtO/eFJLF68GIC5c+dy5OGHsWOP7dl9l95MfuWV2i5vK2ij9dvx6JD/44X7LmDCvb/kjKP7ALD9lp148tZzGXvXQJ6542fssM2my8658meH88pDgxh39y/ovtVGy11vrTVXZ9qoX3P1z48o5NsoCdl8tnVveRgG9K1SNhB4PCK6AI+n1wAHAl3SNgC4HrLgDAwCdgJ6A4MqA3RtHGwbkZYtW/LomCcYN/FFnnt+EqNHPcpzY8dy1DH9efGV13j+hZdZsHABt9x8EwC/u/wyunXrzvgXXuLmW27jp+f8uMjvoHFZsvRLBl51Pz2+92v2Ou4KTv3+nmy1+QYMPvtQBg/5BzsfdTmXXv8wg88+FIADdt+ab2yyHtv2u5gzfz2ca84/arnrDTr9YJ6eMLUYb6UkKI//6hIRTwFVl/zqB9yant8KHJpTfltkxgLtJG0IHACMiYiPImIuMIavB/CvcbBtRCTRunVrABYvXsySxYuRRN8DD0ISkthhh9689950AF579b/02Ttb17PrVlvxzjtv8+GHHxat/Y3NB7M/ZtJr2c/6088X8dpbH9BxvXZEQJs1VwegbetWzJg1H4BD9tqeOx8eB8C4l9+m7Vqt2GDdNgD0+ObGdFinDY89+2oR3klpkOreVtL6ETEDID12SOWdgHdzjpueymoqr5WDbSOzdOlSdurVnU06duBb++5H7512WrZv8eLFDL/jr+x3QPZHeLvtu/HQg/cDMH7cOP73zju8N316Udrd2G2y4dp077oR4195m/OuuJfLzj6UKf+4lN/85DAuvPYhADp2aMf0D+YuO+e9D+fRsUM7JHH5Od/l/KsfKFbzi65yboS6NmBdSc/nbANWsdqqopbyWjVYsJUUkq7Mef1TSRc1QD3nV3n9n/quo5xUVFTw3IRJTH17Os+PH7dcHvbHZ57Obnvsye677wHAT382kHlz57JTr+5cf921dOveg+bNPUClvq3ZqgXDr/gh511xH598tpABR+zBz668ny4H/oqfXXEf1w/qD1TfM4sITj1yD0Y9M5npH84rcMtLST5JBAHMjogdcrYheVz8w5QeID3OTOXTgY1zjtsIeL+W8lo15L+sRcB3Jf0mImY3YD3nA5dVvoiIXRuwrrLRrl079tyrD6NHP8o2227L4EsvZtbsWdx9/V+WHdOmTRuG3HwLkP2j3qrLZnTebLNiNblRat68GcOvOIW7//E8Dz3xIgD9D9mJc393LwD3jXmBP194DJD1ZDfa4KvvWTqt344Zs+az0/absVuPbzDgyD1Ys1VLWqxWwacLFvGra0YU/g0VS8MO/RoBHA9cnh4fyik/U9JdZF+GzY+IGZJGAZflfCm2P/CLuippyDTCEmAI8JOqOyStJ+k+SePTtltO+RhJEyX9RdI7ktZN+x6UNEHS5MqPBpIuB1pJmiTpjlT2aXq8W9JBOXUOk/Q9SRWSfp/qfUnSqQ34MyioWbNmMW9e1vtZsGABTzz+GF27bsUtN9/EmNGjuO324TRr9tWvfN68eXzxxRcA3HLzTey++560adOmKG1vrG4Y1J/X3/qAa25/YlnZjFnz2aNXFwD69N6Sqf+bBcAjT77MMYf0BqD3dp35+NMFfDD7Y0785a1sedCFbHXwIH5x9QPc+fC4phVoE+Wx1XkNaTjwLNBV0nRJJ5MF2f0kTQH2S68BRgJvAlOBG4HTASLiI+BSYHzaLklltWroz4zXAS9J+l2V8j8CV0fEM5I2AUYB3yQbTvFERPxGUl+y4RaVToqIjyS1AsZLui8iBko6MyK6V1P3XcD3gZGSWgD7AKcBJ5P9hdpRUkvg35JGR8RbuSengD4AYONNNlnFH0NhfDBjBqecdDxLly7ly/iS7x1+JAcdfAitV2/OJptuSp/ddwGg32Hf5fwLLuS1V1/lhycdR0VFBVt9c2tuGHJzkd9B47Jr983pf8hOvPzGe4y9KxtNNOhPIzjj0jv5/XmH07x5MxYtWsKZvx4OwKPPTOaA3bdh8ohBfL5wMadedHsxm19S6ms+24g4uoZd+1RzbABn1HCdocDQFalb2fXqn6RPI6K1pEuAxcACoHVEXCRpJsvnONYDtgKeBg6rDHySPgK2jIjZKd97WDq+M3BARIytrKeaelcHpgBbkA3LODIi+ku6F9ge+Dyd0hY4NSJG1/ReevXaIf793POr9gOxetd+xzOL3QSrwcJJ102IiB3q63rf3K5H3PLgP+s8bpct2tdrvfWpEN+G/AGYCNySU9YM2CUiFuQeqBrWvZDUB9g3nfO5pH8Bq9dWaUQsTMcdQNbDHV55OeCsiBi1wu/EzIomn3G0pazBh36lXMY9ZB/fK40GlnVLJFWmAZ4Bjkxl+wOVCei2wNwUaLcCds651mJJq9VQ/V3AicAeZKkK0uNpledI2lLSmiv59sysQBpwnG1BFGqc7ZXAujmv/w/YIX1B9V/gR6n8YmB/SRPJbpWbAXwCPAo0l/QSWWJ6bM61hpDlhe+opt7RwJ7AYxHxRSq7CfgvMDHdH/0XPCGPWckr92DbYEEmN48aER8Ca+S8nk320b6q+WS52CWSdgH2johFad+BNdTzc+DnNdS7GFinyvFfkg0XW258rpmVrmy0QYlH0zqUWo9uE+AeSc2AL4BTitweMysFZdBzrUtJBduImAL0KHY7zKz0lHmsLa1ga2ZWvWwipXLmYGtmZaHMY62DrZmVvnxvxy1lDrZmVh7KPNo62JpZWchnjbFS5mBrZmWhvEOtg62ZlYNGkLR1sDWzsuA7yMzMGljlUublzMHWzMqDg62ZWcNzGsHMrADKfOSXg62ZlQcHWzOzBub5bM3MCsHz2ZqZFUaZx1oHWzMrB57P1sysIMo81jrYmlnpawRTIzjYmlmZKPNo62BrZmXB89mamRVAeYdaB1szKwceZ2tmVijlHW0dbM2s5Hk+WzOzAin3NEKzYjfAzCwfyuO/vK4jvS3pZUmTJD2fytaWNEbSlPTYPpVL0jWSpkp6SVLPlW2/g62ZlQflseVv74joHhE7pNcDgccjogvweHoNcCDQJW0DgOtXtvkOtmZW8qQsZ1vXtgr6Abem57cCh+aU3xaZsUA7SRuuTAUOtmZWFvJMI6wr6fmcbUA1lwpgtKQJOfvXj4gZAOmxQyrvBLybc+70VLbC/AWZmZWH/Hqus3NSAzXZLSLel9QBGCPptRWsNfJqSRXu2ZpZWaivlG1EvJ8eZwIPAL2BDyvTA+lxZjp8OrBxzukbAe+vTPsdbM2sDIhmqnur8yrSmpLWqnwO7A+8AowAjk+HHQ88lJ6PAI5LoxJ2BuZXphtWlNMIZlbyRL2Ns10feCBNRN4cuDMiHpU0HrhH0snA/4Aj0vEjgYOAqcDnwIkrW7GDrZk1GRHxJtCtmvI5wD7VlFAO8sgAAAtsSURBVAdwRn3U7WBrZmWh3O8gc7A1s9Inz2drZtbgvCyOmVmhlHm0dbA1s7LgNIKZWQGUd6h1sDWzclHm0dbB1szKQr7z1ZYqZWN2rTaSZgHvFLsd9WRdYHaxG2HVaky/m00jYr36upikR8l+PnWZHRF966ve+uRg28RIej6PWZGsCPy7adw8EY2ZWQE42JqZFYCDbdMzpNgNsBr5d9OIOWdrZlYA7tmamRWAg62ZWQE42JqZFYCDrZlZATjY2nKkMp9ayaxEeW4EW0aS0ppLSNoXaAM8B3wQEUuL2jgDvvodpeW2Vbkst5U+92xtmZxA+2PgYmAn4AmgdzHbZV9JgfZQYDhwvaTfStqo2O2yujnY2nIkbQnsFRG7AW+TLev8XM5+pxmKSNJ2wDnAIcA4YG9gflEbZXlxsLVlJK0DvA+8JGkYcChwYER8Kel4SW3Dd8EU21LgYeAI4GDgqIj4RNI2xW2W1cXB1gCQtDPwC2AJsAGwBXByRCyR9APgXGCtIjaxSZO0taTDgS+APYDTgeMi4k1JBwI3StqgqI20Wvl23SYopQIUEV/mlG0GPA78kCx18DtgLlAB9AD6R8QrRWiuAZJOAU6IiN0knU2WR38C+Bz4JfDziHi4mG202jnYNkFVRh2sAyyKiE8lfQ/YOyLOlNSFrIe7PjA+IhrL5OllIWfUQUXlSBBJdwLPRsS1kn4IbAqsDTwUEaNzf69Wejz0qwlJPdrtgF8BR0jqBQwE3pY0FBgL9JPUJSKmAFOK19qmKX1B2S0i/iZpB2AvSdMi4kFgKLA/QETclI5fLSIWpzIH2hLmnG0TEpmXgDMl9QEmkQXemcADZLnAbwBXSmpRtIY2bc2AmZLWAt4FWgJnSPoTsBg4SNKxOccvKUIbbSU42DYRklrlvJwNnAi8ArwVEb8Hfkz2kXQR8E1gjYI30oiI14B/kwXaQyPiMuA7ZLnznYF2wPGSWqfj3ZstE87ZNgGSVicbTTCSbJTBdhFxYUod7AJ0j4hFkpoDawLrRMSbxWtx0yJpDWC/iHhI0k5kIw4EPAoMjog/SmpGlkM/ApgaEY8Ur8W2MhxsGzlJ60bEbEl7AE8CU8mC7aK0/xay0QY7R8TCIja1SUvjmncAFgKnRMQLknoCjwEXRMSfqxzvL8PKjNMIjZQyGwO/Th85/ws8BGxI9o8agIg4EZgMPFWUhjZxOXfk/YYsjbMkIl4AiIiJwL7AH9Mt1Ms40JYf92wbOUltgG2BNSNijKRvAQ8Cx0TEw5J2joixkjpExMzitrZpyRne1QxoDbQnG3GwOCL65hzXBegcEWOK1FSrB+7ZNkK58xdExMdAN+BCSX0j4gngB8DfJF0JDJW0kQNtYeUE2v2BC8huu30nIvYBWkj6u6SdJD0JzEl/KD0vRRnzONtGpsoNC8cA8yPiekmLgfPS/hGS9gP2IvvGe3ox29wUpUDbF7gSOBMYLqkb8KuI+Jak4WQzr10ZER9VnlO8FtuqchqhkZJ0Btmtt0emGxQqg+9JwDUp4PpLliJIaYO1gFvJxjmvD/weeA+YB5wVEXMltYuIef49NQ7u2TYy6aPmFsBxZLNCfSDpMGBj4HZgNeBkSY9HxGfFa2nTkxM0V4+I+ZJOJvtS7BKyLy3XAGYA0yVdHBHzwD3axsLBthHI7fmkxykp13cX8BrZP+j5wNoRcZGkhxxoCysnR7sT8GdJJ0TEy5I6kI2rbU/2exoF3BcRC4rZXqt/DrZlrkqOdleyf7STgLvJ5jZ4IiKmSfoRsH06zZNNF1gKtPsB3yW7G2yUpANSwB0H3AF0Bs6MiPFFbKo1EOdsy1TVPJ6knwJHAbOAOcAzwB1pYumTgdPIpujzNIlFkKawHAmcmIbaXQicQJbqmUaWRlgSEeOK10prSO7Zlq/mZBOTkCaNPgDYIyIWpKkS9wC2kTSL7A6xEx1oi2oO8DzZUkNExCVp/OwoYLeI+E8R22YF4HG2ZSh9HL1N0sA0fGgO2aD4PQEi4j6y2aL6RcQ04NyIeLloDW6CKsfESmqrbDmhj8lGIHw357BhwHTgocqJZazxcs+2zKTgegnwV6ADcDTZigp3Ar0lzU0fRScAW6bJpxcVrcFNVMrRfptscca5ksaSLTs0XNlquJ+TLdp4MnAW2QRAnxarvdbw3LMtI5LWJsv7XRoR1wJDgNWBdchmiBJwtaQhZJOC31o5y781vNw7vJSt6XY+cCzZKrinRMSrwJFkvdnWZOOg1wd2Bb782gWtUfEXZGVG0sFk64PtEhEfS7oDeDIihkhqD2xG9q32hPBSNgUjaT2y1YiHR7bE0J5kc8+2JOvdHhMRb0nqHBFvp3N2BW4ju4vP+fRGzmmEMhMRj0j6EpggaRTQiuxmBSJiLllKYWIRm9hU7QbsBLRM0yVWkM3kNYdsOfh5Kdf+ozQMbw7wDrCP/yg2De7ZlilJ+wKjgQ0iYqak1T0fbeGlnPhSSRVkPds+wH/TfBSXkn0hdjjZGOcLgZ954u+mycG2jEk6ELiCbEVcz9pVYJK6kuVdRwNPpdUuDgQOJAu4N0i6iGwO4XbA0IgY5bkOmiYH2zInqR8wiGxQfPgfceFI2gv4J9mdevcAm5NNKLMf0AJ4HxiWRib4k0cT52DbCEhqHREeNlQEknYHHibL136P7Hbpw8hGHGwBXEQ2ITgR4REHTZi/IGsEHGiLJyKekXQ0cC+wa7o9+mFgO2AA2erFDrLmnq1ZfZB0EHAtsGPlZN85M305R2vu2ZrVh4gYmYbkvSapa0TMrTLtpTVx7tma1aN008lnEfGvYrfFSouDrVkDcOrAqnKwNTMrAE9EY2ZWAA62ZmYF4GBrZlYADraWN0lLJU2S9Iqkv0laYxWu1ScN/kfSdyQNrOXYdpJOX4k6Lkprs+VVXuWYYZIOX4G6OkvyNIlWIwdbWxELIqJ7RGxLtvz2j3J3KrPC/09FxIiIuLyWQ9oBKxxszUqJg62trKeBLVKP7lVJfyabR3djSftLelbSxNQDbg3Zkj6SXpP0DDlrcUk6QdKf0vP1JT0g6cW07QpcDnwj9ap/n447T9J4SS9JujjnWr+U9Lqkx4Cudb0JSaek67wo6b4qvfV9JT0t6Q1Jh6TjKyT9PqfuU1f1B2lNg4OtrTBJzcmmEaxcRLIrcFtE9AA+Ay4A9o2InmQryp4jaXXgRuDbZCv/blDD5a8hW3miG9ATmEy2xM+01Ks+T9L+QBegN9Ad6CVpT0m9yJZz70EWzHfM4+3cHxE7pvpeJVsTrFJnYC+y5cZvSO/hZGB+ROyYrn+KsmXKzWrl23VtRbSSNCk9fxq4GegIvBMRY1P5zsDWwL/TklwtgGeBrcgmZZkCIOl2solaqvoWcBxAWj9tflruJ9f+aXshvW5NFnzXAh6IiM9THSPyeE/bSvo1WaqiNdnS4pXuSZPITJH0ZnoP+wPb5+Rz26a638ijLmvCHGxtRSyIiO65BSmgfpZbBIyJiKOrHNcdqK87aAT8JiL+UqWOs1eijmFka4C9KOkEspUWKlW9VqS6z4qI3KCMpM4rWK81MU4jWH0bC+wmaQsASWtI2hJ4DdhM0jfScUfXcP7jwGnp3ApJbYBPyHqtlUYBJ+XkgjtJ6gA8BRwmqZWktchSFnVZC5ghaTWgf5V9R0hqltq8OfB6qvu0dDyStpS0Zh71WBPnnq3Vq4iYlXqIwyW1TMUXRMQbkgYAj0iaDTwDbFvNJX4MDJF0MrAUOC0inpX07zS06h8pb/tN4NnUs/4U+EFETJR0NzCJbDHFp/No8q+A59LxL7N8UH8deJJsufEfRcRCSTeR5XInKqt8FtnaY2a18twIZmYF4DSCmVkBONiamRWAg62ZWQE42JqZFYCDrZlZATjYmpkVgIOtmVkB/D+iSNEg0bdfpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_log_test)\n",
    "classes = ['Positive', 'Negative']\n",
    "plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get coefficient values for features and create dict with feature and importance\n",
    "dictionary_logreg = dict(zip(list(X_train.columns), list(logreg.coef_[0])))\n",
    "\n",
    "#sort dictionary by most important features\n",
    "dictionary_logreg = {k: v for k, v in sorted(dictionary_logreg.items(), key=lambda item: abs(item[1]))}\n",
    "\n",
    "# make dictionary a list with most important features at top\n",
    "importance = dictionary_logreg.items()\n",
    "impt_features = list(importance)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('waste', -0.6369390978180243),\n",
       " ('bad', -0.5942151784481359),\n",
       " ('excellent', 0.5282746064058913),\n",
       " ('awful', -0.5083091328531034),\n",
       " ('boring', -0.44226076377710194),\n",
       " ('great', 0.41382745120610454),\n",
       " ('poor', -0.41025905355035314),\n",
       " ('favorite', 0.40398783345962336),\n",
       " ('perfect', 0.3859690539725842),\n",
       " ('amazing', 0.36910496961797357)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top ten most important features for logistic regression model\n",
    "impt_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (spoiler) pickling our best model with BOW vectorization\n",
    "pickle_out = open(\"log_reg_mod.pickle\",\"wb\")\n",
    "pickle.dump(logreg, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy score:  0.92544\n",
      "Test Accuracy score:  0.87376\n",
      "\n",
      "Train F1 score:  0.9254479522184301\n",
      "Test F1 score:  0.8743030109925124\n",
      "\n",
      "Confusion matrix test set: \n",
      " [[0.43472 0.064  ]\n",
      " [0.06224 0.43904]]\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATE NB Classifier\n",
    "Naive = MultinomialNB(alpha=.1, fit_prior= True)\n",
    "\n",
    "#fit to training set\n",
    "Naive.fit(X_train,y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_nb_train = Naive.predict(X_train)\n",
    "y_pred_nb_test = Naive.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_nb_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_nb_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_nb_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_nb_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_nb_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSATNTIATE DECISION TREE MODEL\n",
    "tree = DecisionTreeClassifier(class_weight = 'balanced',random_state = 333)\n",
    "\n",
    "#fit to training set\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_tree_train = tree.predict(X_train)\n",
    "y_pred_tree_test = tree.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_tree_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_tree_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_tree_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_tree_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_tree_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE RANDOM FOREST MODEL\n",
    "rfc = RandomForestClassifier(random_state = 333, class_weight='balanced')\n",
    "\n",
    "#fit to training set\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_rfc_train = rfc.predict(X_train)\n",
    "y_pred_rfc_test = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_rfc_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_rfc_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_rfc_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_rfc_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_rfc_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking to prioritze accuracy score in our model but want to take into account F1 if there is a negligible difference in accuracy. We will pick our two best performing models to try and improve upon, those being Logistic Regression and Naive Bayes. In the following steps we will tune our parameters and see if we can get improve our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PARAMATER GRID\n",
    "param_grid_log =  {'C': np.logspace(-4, 4, 20),\n",
    "                  'fit_intercept': [True, False],\n",
    "                  'max_iter': [1000,10_000]}\n",
    "grid_search_log = GridSearchCV(estimator = logreg, param_grid = param_grid_log ,cv = 7,scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_log.fit(X_train,y_train)\n",
    "log_params = grid_search_log.best_params_\n",
    "print(log_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = LogisticRegression(**log_params, class_weight='balanced', random_state = 333)\n",
    "\n",
    "logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_grid = logreg_grid.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_log_grid))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_log_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PARAMATER GRID\n",
    "param_grid_nb =   {'alpha': [1,0.5],\n",
    "                   'fit_prior': [True, False]}\n",
    "grid_search_nb = GridSearchCV(estimator = Naive, param_grid = param_grid_nb,cv = 7,scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_nb.fit(X_train,y_train)\n",
    "nb_params = grid_search_nb.best_params_\n",
    "print(nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = MultinomialNB(**nb_params, class_weight='balanced', random_state = 333)\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb_grid = nb_grid.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_nb_grid))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_nb_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best BOW model with updated stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in most common words from Unigram analysis\n",
    "stopwords_new = stopwords_list + ['movie', 'film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vectorize our cleaned reviews using CountVectorizer and updated stopwords\n",
    "X_train, X_test, y_train, y_test, BOW_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], CountVectorizer, stopwords_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#INSTANTIATE LOGISTIC REGRESSION\n",
    "logreg = LogisticRegression(C = .01, random_state = 333)\n",
    "# can choose a regularization C = ? to penalize so it doesnt overfit\n",
    "\n",
    "#fit to training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_log_train = logreg.predict(X_train)\n",
    "y_pred_log_test = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_log_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_log_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_log_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_log_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_log_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vecotrize our cleaned reviews using CountVectorizer\n",
    "X_train, X_test, y_train, y_test, TFIDF_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], TfidfVectorizer, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"TFIDF_vect.pickle\",\"wb\")\n",
    "pickle.dump(TFIDF_vect, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE LOGISTIC REGRESSION\n",
    "logreg = LogisticRegression(C = .01, random_state = 333)\n",
    "# can choose a regularization C = ? to penalize so it doesnt overfit\n",
    "\n",
    "#fit to training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_log_train = logreg.predict(X_train)\n",
    "y_pred_log_test = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_log_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_log_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_log_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_log_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_log_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE NB Classifier\n",
    "Naive = MultinomialNB(alpha=0.1, fit_prior= True)\n",
    "\n",
    "#fit to training set\n",
    "Naive.fit(X_train,y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_nb_train = Naive.predict(X_train)\n",
    "y_pred_nb_test = Naive.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_nb_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_nb_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_nb_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_nb_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_nb_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"nb_mod.pickle\",\"wb\")\n",
    "pickle.dump(Naive, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSATNTIATE DECISION TREE MODEL\n",
    "tree = DecisionTreeClassifier(class_weight = 'balanced',random_state = 333)\n",
    "\n",
    "#fit to training set\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_tree_train = tree.predict(X_train)\n",
    "y_pred_tree_test = tree.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_tree_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_tree_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_tree_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_tree_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_tree_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE RANDOM FOREST MODEL\n",
    "rfc = RandomForestClassifier(random_state = 333, class_weight='balanced')\n",
    "\n",
    "#fit to training set\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_rfc_train = rfc.predict(X_train)\n",
    "y_pred_rfc_test = rfc.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_rfc_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_rfc_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_rfc_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_rfc_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_rfc_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PARAMATER GRID\n",
    "param_grid_log =  {'C': np.logspace(-4, 4, 20),\n",
    "                  'fit_intercept': [True, False],\n",
    "                  'max_iter': [1000,10_000]}\n",
    "grid_search_log = GridSearchCV(estimator = logreg, param_grid = param_grid_log ,cv = 7,scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vectorize our cleaned reviews using updated stopwords\n",
    "X_train, X_test, y_train, y_test, TFIDF_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], TfidfVectorizer, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_log.fit(X_train,y_train)\n",
    "log_params = grid_search_log.best_params_\n",
    "print(log_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_grid = LogisticRegression(**log_params, class_weight='balanced', random_state = 333)\n",
    "\n",
    "logreg_grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_log_grid = logreg_grid.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_log_grid))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_log_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET PARAMATER GRID\n",
    "param_grid_nb =   {'alpha': [1,0.5],\n",
    "                   'fit_prior': [True, False]}\n",
    "grid_search_nb = GridSearchCV(estimator = Naive, param_grid = param_grid_nb,cv = 7,scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vectorize our cleaned reviews using updated stopwords\n",
    "X_train, X_test, y_train, y_test, TFIDF_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], TfidfVectorizer, stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_nb.fit(X_train,y_train)\n",
    "nb_params = grid_search_nb.best_params_\n",
    "print(nb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid = MultinomialNB(**nb_params, class_weight='balanced', random_state = 333)\n",
    "\n",
    "nb_grid.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb_grid = nb_grid.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_nb_grid))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_nb_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best TFIDF Model with updated stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split and vectorize our cleaned reviews using updated stopwords\n",
    "X_train, X_test, y_train, y_test, TFIDF_vect = split_vectorize(\n",
    "    df['cleaned_reviews'], df['sentiment'], TfidfVectorizer, stopwords_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE NB Classifier\n",
    "Naive = MultinomialNB(alpha=0.1, fit_prior= True)\n",
    "\n",
    "#fit to training set\n",
    "Naive.fit(X_train,y_train)\n",
    "\n",
    "#predict on train and test set\n",
    "y_pred_nb_train = Naive.predict(X_train)\n",
    "y_pred_nb_test = Naive.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Train Accuracy score: ', accuracy_score(y_pred_nb_train, y_train))\n",
    "print('Test Accuracy score: ', accuracy_score(y_pred_nb_test, y_test))\n",
    "print()\n",
    "# checking F1\n",
    "print('Train F1 score: ', f1_score(y_pred_nb_train, y_train))\n",
    "print('Test F1 score: ', f1_score(y_pred_nb_test,y_test))\n",
    "print()\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_nb_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reviews_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment\n",
       "0  Working with one of the best Shakespeare sourc...         -1\n",
       "1  Well...tremors I, the original started off in ...         -1\n",
       "2  Ouch! This one was a bit painful to sit throug...         -1\n",
       "3  I've seen some crappy movies in my life, but t...         -1\n",
       "4  \"Carriers\" follows the exploits of two guys an...         -1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  0000000000001  00001  00015  001  007  00am  00pm  00s  ...  \\\n",
      "0   0    0              0      0      0    0    0     0     0    0  ...   \n",
      "1   0    0              0      0      0    0    0     0     0    0  ...   \n",
      "2   0    0              0      0      0    0    0     0     0    0  ...   \n",
      "3   0    0              0      0      0    0    0     0     0    0  ...   \n",
      "4   0    0              0      0      0    0    0     0     0    0  ...   \n",
      "\n",
      "   zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  \\\n",
      "0                                0                                          0   \n",
      "1                                0                                          0   \n",
      "2                                0                                          0   \n",
      "3                                0                                          0   \n",
      "4                                0                                          0   \n",
      "\n",
      "   zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  ánd  åmål  émigré  évery  \\\n",
      "0                                             0    0     0       0      0   \n",
      "1                                             0    0     0       0      0   \n",
      "2                                             0    0     0       0      0   \n",
      "3                                             0    0     0       0      0   \n",
      "4                                             0    0     0       0      0   \n",
      "\n",
      "   ísnt  über  üvegtigris  \n",
      "0     0     0           0  \n",
      "1     0     0           0  \n",
      "2     0     0           0  \n",
      "3     0     0           0  \n",
      "4     0     0           0  \n",
      "\n",
      "[5 rows x 53943 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the stop words\n",
    "#my_stop_words = ENGLISH_STOP_WORDS.union(['movie','film', etc])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "vect.fit(df['reviews'])\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(df['reviews'])\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   10  acting  action  actors  actually  away  awful  bad  believe  best  ...  \\\n",
      "0   0       0       0       0         0     0      0    0        0     1  ...   \n",
      "1   0       0       0       0         0     0      0    2        0     0  ...   \n",
      "2   0       0       0       1         0     1      0    0        0     0  ...   \n",
      "3   1       0       0       0         0     0      0    0        0     2  ...   \n",
      "4   0       0       1       0         0     1      1    0        0     0  ...   \n",
      "\n",
      "   trying  ve  want  wasn  watch  watching  way  work  worst  years  \n",
      "0       0   0     0     0      0         0    0     0      0      0  \n",
      "1       0   0     0     0      1         0    0     0      0      0  \n",
      "2       0   0     0     0      0         0    2     1      0      0  \n",
      "3       1   1     0     0      0         0    0     0      1      0  \n",
      "4       0   0     0     0      0         0    0     0      1      0  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100,stop_words=ENGLISH_STOP_WORDS)\n",
    "# can add in ngram_range=(1,2) for uni/bigram analysis, max_df = 200 (limits size of vocab to which occurs in \n",
    "# no more than 200 docs), min_df = 50 (limit size of vocab to ignore terms which occur in less than 50 docs)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vect.fit(df['reviews'])\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(df['reviews'])\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenization (feature engineering - show how many words are in a review, how many sentences, how many punctuation marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in df['reviews']]\n",
    "\n",
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "df['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working with one of the best Shakespeare sourc...</td>\n",
       "      <td>-1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well...tremors I, the original started off in ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ouch! This one was a bit painful to sit throug...</td>\n",
       "      <td>-1</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've seen some crappy movies in my life, but t...</td>\n",
       "      <td>-1</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Carriers\" follows the exploits of two guys an...</td>\n",
       "      <td>-1</td>\n",
       "      <td>940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  sentiment  n_words\n",
       "0  Working with one of the best Shakespeare sourc...         -1       55\n",
       "1  Well...tremors I, the original started off in ...         -1      220\n",
       "2  Ouch! This one was a bit painful to sit throug...         -1      146\n",
       "3  I've seen some crappy movies in my life, but t...         -1      481\n",
       "4  \"Carriers\" follows the exploits of two guys an...         -1      940"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the column of reviews to tokens\n",
    "tokens = [word_tokenize(review) for review in df['reviews']]\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in review] for review in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1ad4098382b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Transform the column of reviews to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Stem the list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWNlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-1ad4098382b7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Transform the column of reviews to tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Stem the list of tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlemmed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWNlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     return [\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Call the lemmer\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Transform the column of reviews to tokens\n",
    "tokens = [word_tokenize(review) for review in df['reviews']]\n",
    "# Stem the list of tokens\n",
    "lemmed_tokens = [[WNlemmatizer.lemmatize(word) for word in review] for review in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actors</th>\n",
       "      <th>actually</th>\n",
       "      <th>away</th>\n",
       "      <th>awful</th>\n",
       "      <th>bad</th>\n",
       "      <th>believe</th>\n",
       "      <th>best</th>\n",
       "      <th>...</th>\n",
       "      <th>trying</th>\n",
       "      <th>ve</th>\n",
       "      <th>want</th>\n",
       "      <th>wasn</th>\n",
       "      <th>watch</th>\n",
       "      <th>watching</th>\n",
       "      <th>way</th>\n",
       "      <th>work</th>\n",
       "      <th>worst</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.263806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.449483</td>\n",
       "      <td>0.279081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.084258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091223</td>\n",
       "      <td>0.077849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078806</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138168</td>\n",
       "      <td>0.137611</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118349</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         10  acting    action    actors  actually      away     awful     bad  \\\n",
       "0  0.000000     0.0  0.000000  0.000000       0.0  0.000000  0.000000  0.0000   \n",
       "1  0.000000     0.0  0.000000  0.000000       0.0  0.000000  0.000000  0.2598   \n",
       "2  0.000000     0.0  0.000000  0.263806       0.0  0.303831  0.000000  0.0000   \n",
       "3  0.084258     0.0  0.000000  0.000000       0.0  0.000000  0.000000  0.0000   \n",
       "4  0.000000     0.0  0.144418  0.000000       0.0  0.138168  0.137611  0.0000   \n",
       "\n",
       "   believe      best  ...    trying        ve  want  wasn    watch  watching  \\\n",
       "0      0.0  0.446175  ...  0.000000  0.000000   0.0   0.0  0.00000       0.0   \n",
       "1      0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.15893       0.0   \n",
       "2      0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.00000       0.0   \n",
       "3      0.0  0.166486  ...  0.091223  0.077849   0.0   0.0  0.00000       0.0   \n",
       "4      0.0  0.000000  ...  0.000000  0.000000   0.0   0.0  0.00000       0.0   \n",
       "\n",
       "        way      work     worst  years  \n",
       "0  0.000000  0.000000  0.000000    0.0  \n",
       "1  0.000000  0.000000  0.000000    0.0  \n",
       "2  0.449483  0.279081  0.000000    0.0  \n",
       "3  0.000000  0.000000  0.078806    0.0  \n",
       "4  0.000000  0.000000  0.118349    0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the vectorizer and specify the arguments\n",
    "vect = TfidfVectorizer(ngram_range=(1, 2), max_features=100, stop_words=ENGLISH_STOP_WORDS).fit(df['reviews'])\n",
    "# can also specify our own token_pattern using regex\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(df['reviews'])\n",
    "\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns to drop from df\n",
    "drop = []\n",
    "\n",
    "# choose X, y\n",
    "predictors = movies.drop(columns = drop)\n",
    "target = movies['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN/TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, random_state=333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE LOGISTIC REGRESSION\n",
    "logreg = LogisticRegression(class_weight='balanced', random_state = 333)\n",
    "# can choose a regularization C = ? to penalize so it doesnt overfit\n",
    "\n",
    "#fit to training set\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#predict on test set\n",
    "y_pred_log = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_log))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_log))\n",
    "\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_log)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSATNTIATE DECISION TREE MODEL\n",
    "tree = DecisionTreeClassifier(class_weight = 'balanced',random_state = 333)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_tree))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_tree))\n",
    "\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_tree)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE RANDOM FOREST MODEL\n",
    "rfc = RandomForestClassifier(random_state = 333, class_weight='balanced')\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_forest = rfc.predict(X_test)\n",
    "\n",
    "# checking accuracy\n",
    "print('Test Accuracy score: ', metrics.accuracy_score(y_test, y_pred_forest))\n",
    "\n",
    "# checking F1\n",
    "print('Test F1 score: ', metrics.f1_score(y_test, y_pred_forest))\n",
    "# print confusion matrix\n",
    "print('Confusion matrix test set: \\n', confusion_matrix(y_test, y_pred_forest)/len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
